{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from image_augmentation import random_augmentation\n",
    "import cv2\n",
    "\n",
    "class GDataset(Dataset):\n",
    "    def __init__(self, df, directory):\n",
    "\n",
    "        self.df = df\n",
    "        self.dir = directory\n",
    "        self.transform = tt.Compose([\n",
    "            tt.Resize((128, 128)),\n",
    "            tt.ToTensor(),\n",
    "            tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "    def randBrightness(self, image):\n",
    "        brightness = np.random.uniform(0.7, 1.3)\n",
    "        jitter = brightness * np.array(image).astype(np.float32)\n",
    "        jitter = np.clip(jitter, 0, 255).astype(np.uint8)\n",
    "        jittered = Image.fromarray(jitter)\n",
    "        return jittered\n",
    "    \n",
    "    def randCrop(self, image):\n",
    "        code = np.random.randint(0, 5)\n",
    "        scale = np.random.uniform(0.2, 0.4)\n",
    "        width, height = image.size\n",
    "\n",
    "        img_crop = np.array(image)\n",
    "\n",
    "        if code == 0:\n",
    "            img_crop = img_crop[int(height*scale):, :, :]\n",
    "\n",
    "        elif code == 1:\n",
    "            img_crop = img_crop[0:int(-height*scale), :, :]\n",
    "\n",
    "        elif code == 2:\n",
    "            img_crop = img_crop[:,int(width*scale):, :]\n",
    "\n",
    "        elif code == 3:\n",
    "            img_crop = img_crop[:, 0:int(-width*scale), :]\n",
    "\n",
    "        # Upsample to original shape\n",
    "        img_crop = cv2.resize(img_crop, (width, height))\n",
    "\n",
    "        return Image.fromarray(img_crop)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df) * 3\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index, aug = index // 3, index % 3\n",
    "        id_ = self.df.iloc[index]['id']\n",
    "        img_name = \"/\".join([self.dir, id_[:3], id_+\".jpg\"])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if aug == 1:\n",
    "            image = self.randCrop(image)\n",
    "        elif aug == 2:\n",
    "            image = self.randBrightness(image)\n",
    "        image = self.transform(image)\n",
    "        return image, torch.tensor(int(self.df.iloc[index]['class_id'])).long()\n",
    "\n",
    "    \n",
    "class InferenceDataset(GDataset):\n",
    "    def __init__(self, df, directory):\n",
    "        super().__init__(df, directory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id_ = self.df.iloc[index]['id']\n",
    "        img_name = \"/\".join([self.dir, id_[:3], id_+\".jpg\"])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        return image, torch.tensor(int(self.df.iloc[index]['class_id'])).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from vgg16 import VGG16\n",
    "import sys\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def train_model(index, dir_, model_file=\"\", \n",
    "                epochs=12, \n",
    "                model_save=False, model_save_name=\"vgg16_iter\"):\n",
    "    \"\"\" Training the model\n",
    "        @param index: index file of the train data\n",
    "        @param dir_: directory name that stores the properly formatted data\n",
    "        @param model_file: model state file for continuous training\n",
    "        @param epochs: number of iterations to run\n",
    "        @param model_save: whether to save as a model file\n",
    "        @param model_save_name: name of the saving model file\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(index)\n",
    "    df = df.loc[df.class_id < 100]\n",
    "    dataset = GDataset(df, dir_)\n",
    "    \n",
    "    batch_size = 108\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.avgpool = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(output_size=(3, 3)))\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "                torch.nn.Linear(4608, 4096),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(p=0.5),\n",
    "                torch.nn.Linear(4096, 4096),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(4096, 100))\n",
    "    \n",
    "    for param in model.features[:34].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "        # train the model\n",
    "    for e in range(epochs):\n",
    "\n",
    "        train_loss, valid_loss = 0, 0\n",
    "        accuracy, accuracy_v = 0, 0\n",
    "        counter = 0\n",
    "        for x, t in dataloader:\n",
    "            counter += 1\n",
    "            print(round(counter / len(dataloader) * 100, 2), \"%  \", end=\"\\r\")\n",
    "            x, t = x.cuda(), t.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            y = softmax(z)\n",
    "            top_p, top_class = y.topk(1, dim=1)\n",
    "            accuracy += (top_class[:, 0] == t).sum().item()\n",
    "            \n",
    "\n",
    "        print(e, \n",
    "              train_loss / len(dataset), \n",
    "              accuracy / len(dataset))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def inference(index, dir_, model):\n",
    "    \"\"\" Inferencing the model\n",
    "        @param index: index file of the train data\n",
    "        @param dir_: directory name that stores the properly formatted data\n",
    "        @param model_file: model state file for continuous training\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(index)\n",
    "    df = df.loc[df.class_id < 100]\n",
    "    dataset = InferenceDataset(df, dir_)\n",
    "    \n",
    "    # model = VGG16()\n",
    "    # model.load_state_dict(torch.load(model_file))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    transform = tt.Compose([\n",
    "            tt.Resize((128, 128)),\n",
    "            tt.ToTensor(),\n",
    "            tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    loss_sum = 0\n",
    "    accuracy = 0\n",
    "    counter = 0\n",
    "    for x, t_cpu in dataset:\n",
    "        counter += 1\n",
    "        x_cpu = np.array(x)\n",
    "        x, t = transform(x).unsqueeze(0).cuda(), t_cpu.unsqueeze(0).cuda()\n",
    "        z = model(x)\n",
    "        y = softmax(z)\n",
    "        top_p, top_class = y.topk(5, dim=1)\n",
    "        accuracy += (top_class[:, 0] == t).sum().item()\n",
    "            \n",
    "\n",
    "    print(\"Top 1 Accuracy:\", accuracy / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.028101516308267013 0.27268754552075747\n",
      "1 0.02104456410821302 0.4229667394998786\n",
      "2 0.017585092836515683 0.4998300558387958\n",
      "3 0.015168939762018236 0.5599417334304443\n",
      "4 0.0128095516796163 0.6181111920369021\n",
      "5 0.011044324937738473 0.6674435542607429\n",
      "6 0.009911194224472213 0.6920611798980335\n",
      "7 0.00886769620328599 0.7270211216314639\n",
      "8 0.007991505981678233 0.7543092983733916\n",
      "9 0.0072619308847106465 0.7736829327506677\n",
      "10 0.006584201231834002 0.7939791211459092\n",
      "11 0.006237845715749434 0.8082544306870599\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\"train_try.csv\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy: 0.5145631067961165\n"
     ]
    }
   ],
   "source": [
    "inference(\"valid_try.csv\", \"train\", model)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
